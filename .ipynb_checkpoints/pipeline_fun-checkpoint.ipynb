{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Fun\n",
    "In this notebook I'll be walking through the Pipeline class and make_pipeline method in scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Binarizer, PolynomialFeatures, \\\n",
    "StandardScaler, FunctionTransformer, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion, \\\n",
    "make_union\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import iris dataset and see shape and head\n",
    "df_iris = pd.read_csv('./iris.csv')\n",
    "\n",
    "print(df_iris.shape)\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoder Mapping: {'setosa': 0, 'versicolor': 1, 'virginica': 2}\n"
     ]
    }
   ],
   "source": [
    "# Setup feature matrix and target vector\n",
    "X = df_iris.drop('species', axis=1)\n",
    "y = df_iris.species\n",
    "\n",
    "# Label encode target vector and print label mapping\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "le_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print('Label Encoder Mapping:', le_mapping)\n",
    "y = le.transform(y)\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\\\n",
    "                                                   random_state=1337)\n",
    "\n",
    "# Use standard scaler\n",
    "# ss = StandardScaler()\n",
    "# X_train = ss.fit_transform(X_train)\n",
    "# X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Class Walkthrough \n",
    "We'll walk through scikit learn's Pipeline class below. First we'll define a feature extractor class that extracts a given column in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Feature Extractor class\n",
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[[self.column]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now define the modeling steps in our pipeline. For this example we'll take the petal length feature, bin it into a dummy variable where the cutoff is the median petal length, then predict the flower species using that one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('extract-petal_length', FeatureExtractor(column='petal_length')), ('cut_off_at_median', Binarizer(copy=True, threshold=4.4)), ('predict_using_knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'))]\n"
     ]
    }
   ],
   "source": [
    "# Define modeling steps\n",
    "modeling_steps = [\n",
    "    ('extract-petal_length', FeatureExtractor('petal_length')),\n",
    "    ('cut_off_at_median', Binarizer(X_train['petal_length'].median())),\n",
    "    ('predict_using_knn', KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "print(modeling_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('extract-petal_length', FeatureExtractor(column='petal_length')), ('cut_off_at_median', Binarizer(copy=True, threshold=4.4)), ('predict_using_knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate Pipeline object with steps and fit\n",
    "model_1 = Pipeline(modeling_steps)\n",
    "model_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this Pipeline object, we can pass in different dataframes and make predictions as long as it contains the 'petal_length' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.533333333333\n",
      "Test score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Score on training and test set\n",
    "print('Training score:', model_1.score(X_train, y_train))\n",
    "print('Test score:', model_1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Get first 5 predictions\n",
    "print(model_1.predict(X_test)[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abalone Dataset\n",
    "We'll run another Pipeline class for this abalone dataset. The data can be found at http://archive.ics.uci.edu/ml/datasets/Abalone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4177, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_weight</th>\n",
       "      <th>shucked_weight</th>\n",
       "      <th>viscera_weight</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sex  length  diameter  height  whole_weight  shucked_weight  viscera_weight  \\\n",
       "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   shell_weight  rings  \n",
       "0         0.150     15  \n",
       "1         0.070      7  \n",
       "2         0.210      9  \n",
       "3         0.155     10  \n",
       "4         0.055      7  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CSV doesn't contain column header so we'll have to create a list\n",
    "# of the columns given in the url\n",
    "abalone_cols = ['sex', 'length', 'diameter', 'height', 'whole_weight',\\\n",
    "               'shucked_weight', 'viscera_weight', 'shell_weight', 'rings']\n",
    "\n",
    "df_abalone = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data',\\\n",
    "                names=abalone_cols)\n",
    "\n",
    "print(df_abalone.shape)\n",
    "df_abalone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4177 entries, 0 to 4176\n",
      "Data columns (total 9 columns):\n",
      "sex               4177 non-null object\n",
      "length            4177 non-null float64\n",
      "diameter          4177 non-null float64\n",
      "height            4177 non-null float64\n",
      "whole_weight      4177 non-null float64\n",
      "shucked_weight    4177 non-null float64\n",
      "viscera_weight    4177 non-null float64\n",
      "shell_weight      4177 non-null float64\n",
      "rings             4177 non-null int64\n",
      "dtypes: float64(7), int64(1), object(1)\n",
      "memory usage: 293.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check for Nans and datatypes\n",
    "df_abalone.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example let's try to predict whether or not the abalone in question is above the average age in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up feature matrix and target vector\n",
    "X = df_abalone.drop('rings', axis=1)\n",
    "y = df_abalone.rings\n",
    "\n",
    "# Binarize target vector\n",
    "y = y.map(lambda x: 1 if x > y.mean() else 0)\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\\\n",
    "                                                   random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3341.000000\n",
      "mean        0.503741\n",
      "std         0.500061\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         1.000000\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "Name: rings, dtype: float64\n",
      "\n",
      "baseline: 50.37%\n"
     ]
    }
   ],
   "source": [
    "# Rings column summary statistics\n",
    "print(y_train.describe())\n",
    "\n",
    "print('')\n",
    "print('baseline: {}%'.format(round(y_train.mean()*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('extract_diameter', FeatureExtractor(column='diameter')), ('create_polynomials', PolynomialFeatures(degree=3, include_bias=False, interaction_only=False)), ('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('predict', LogisticRegression(C=1.0, class_weight=None, dual...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modeling steps\n",
    "modeling_steps = [\n",
    "    ('extract_diameter', FeatureExtractor('diameter')),\n",
    "    ('create_polynomials', PolynomialFeatures(3, include_bias=False)),\n",
    "    ('standardize', StandardScaler()),\n",
    "    ('predict', LogisticRegression())\n",
    "]\n",
    "\n",
    "# Instantiate Pipeline\n",
    "pipe = Pipeline(modeling_steps)\n",
    "\n",
    "# Fit pipeline object\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72368421052631582"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score against test set\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted 0  Predicted 1\n",
      "Actual 0          309          129\n",
      "Actual 1          102          296\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.71      0.73       438\n",
      "          1       0.70      0.74      0.72       398\n",
      "\n",
      "avg / total       0.73      0.72      0.72       836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = pipe.predict(X_test)\n",
    "\n",
    "cf_matrix = pd.DataFrame(confusion_matrix(y_test, pred), \\\n",
    "                        columns=['Predicted 0', 'Predicted 1'], \\\n",
    "                        index=['Actual 0', 'Actual 1'])\n",
    "\n",
    "print(cf_matrix)\n",
    "print('')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For just including polynomial features of one dimension, this model performed fairly well! We beat the baseline by ~20% and the precision and recall scores are fairly balanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make_pipeline()\n",
    "We'll now walkthrough the make_pipeline method in scitkit learn. Big difference between this method and the previous Pipeline class is that we can skip the step where we name the steps of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72368421052631582"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pipeline object\n",
    "pipe = make_pipeline(\n",
    "    FeatureExtractor('diameter'),\n",
    "    PolynomialFeatures(3, include_bias=False),\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "# Fit pipe object\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Score against test set\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the same score as with the Pipeline class which was expected. <br/><br/>\n",
    "If we wanted to one hot encode a categorical column, we would need to create a 'categorical extractor' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CategoricalExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "        self.values = None\n",
    "        \n",
    "    def _create_values(self, indices):\n",
    "        return {ind: i+1 for i, ind in enumerate(indices)}\n",
    "    \n",
    "    def _apply_values(self, row_val):\n",
    "        return self.values.get(row_val, 0)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.values = self._create_values(X[self.column].value_counts().index)\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        col = X[self.column].apply(self._apply_values)\n",
    "        return col.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this class instead of FeatureExtractor to pass it directly to OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M    1228\n",
      "I    1071\n",
      "F    1042\n",
      "Name: sex, dtype: int64\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Show unique categories of sex and show dummified matrix\n",
    "pipe = make_pipeline(\n",
    "    CategoricalExtractor('sex'),\n",
    "    OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    ")\n",
    "\n",
    "print(X_train['sex'].value_counts())\n",
    "pipe.fit(X_train)\n",
    "print(pipe.transform(X_train)[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FeatureUnion() \n",
    "Pipeline is great to manipulate a single column, but what if we wanted to manipulate multiple columns? (Which is more often than not)<br/>\n",
    "Feature Union to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.2 ,  0.04],\n",
       "       [ 0.  ,  0.2 ,  0.04],\n",
       "       [ 0.  ,  0.2 ,  0.04],\n",
       "       [ 0.  ,  0.2 ,  0.04],\n",
       "       [ 0.  ,  0.2 ,  0.04]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline for petal length\n",
    "petal_length_pipe = make_pipeline(\n",
    "    FeatureExtractor('petal_length'),\n",
    "    Binarizer(df_iris['petal_length'].mean())\n",
    ")\n",
    "\n",
    "# pipeline for petal width\n",
    "petal_width_pipe = make_pipeline(\n",
    "    FeatureExtractor('petal_width'),\n",
    "    PolynomialFeatures(2, include_bias=False)\n",
    "    \n",
    ")\n",
    "\n",
    "# Create feature union for the 2 pipelines above\n",
    "fu = FeatureUnion([\n",
    "    ('petal_length_transformer', petal_length_pipe),\n",
    "    ('petal_width_transformer', petal_width_pipe)\n",
    "])\n",
    "\n",
    "# Fit and transform feature union\n",
    "fu.fit(df_iris)\n",
    "fu.transform(df_iris)[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like Pipeline, FeatureUnion has a function that removes some of the boilerplate code (make_union)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.2 ,  0.04],\n",
       "       [ 0.  ,  0.2 ,  0.04],\n",
       "       [ 0.  ,  0.2 ,  0.04],\n",
       "       [ 0.  ,  0.2 ,  0.04],\n",
       "       [ 0.  ,  0.2 ,  0.04]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fu = make_union(\n",
    "    petal_length_pipe,\n",
    "    petal_width_pipe\n",
    ")\n",
    "\n",
    "fu.fit(df_iris)\n",
    "fu.transform(df_iris)[0:5, :]\n",
    "# As expected it's identical to the above feature union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try creating another feature union, but with the abalone dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ,  0.26    ,  0.0676  ,  0.      ],\n",
       "       [ 0.      ,  0.295   ,  0.087025,  0.      ],\n",
       "       [ 1.      ,  0.42    ,  0.1764  ,  1.      ],\n",
       "       [ 1.      ,  0.45    ,  0.2025  ,  1.      ],\n",
       "       [ 1.      ,  0.43    ,  0.1849  ,  1.      ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_mean = X_train.length.mean()\n",
    "\n",
    "length_pipe = make_pipeline(\n",
    "    FeatureExtractor('length'),\n",
    "    Binarizer(length_mean)\n",
    ")\n",
    "\n",
    "diameter_pipe = make_pipeline(\n",
    "    FeatureExtractor('diameter'),\n",
    "    PolynomialFeatures(2, include_bias=False)\n",
    ")\n",
    "\n",
    "height_pipe = make_pipeline(\n",
    "    FeatureExtractor('height'),\n",
    "    Binarizer(0.1)\n",
    ")\n",
    "\n",
    "fu_abalone = make_union(\n",
    "    length_pipe,\n",
    "    diameter_pipe,\n",
    "    height_pipe\n",
    ")\n",
    "\n",
    "fu_abalone.fit(X_train)\n",
    "fu_abalone.transform(X_train)[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected there should be 4 columns from the transformation- binarized length, 2 columns for polynomial features, and binarized height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of pipelines and feature unions is that you can chain them together. We'll build a chained example next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pipeline-1', Pipeline(steps=[('categoricalextractor', CategoricalExtractor(column='sex')), ('onehotencoder', OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='ignore', n_values='au...imators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pipelines for OHE sex column and polynomial features with\n",
    "# diameter column\n",
    "sex_pipe = make_pipeline(\n",
    "    CategoricalExtractor('sex'),\n",
    "    OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    ")\n",
    "\n",
    "diameter_pipe = make_pipeline(\n",
    "    FeatureExtractor('diameter'),\n",
    "    PolynomialFeatures(2, include_bias=False)\n",
    ")\n",
    "\n",
    "# Create feature union for pipelines and extract width and length \n",
    "feature_transformers = make_union(\n",
    "    sex_pipe,\n",
    "    diameter_pipe,\n",
    "    FeatureExtractor('height'),\n",
    "    FeatureExtractor('length')\n",
    ")\n",
    "\n",
    "# Create modeling pipeline with feature union, standard scaler, and\n",
    "# random forest classifier\n",
    "modeling_pipe = make_pipeline(\n",
    "    feature_transformers,\n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "# Fit modeling pipeline\n",
    "modeling_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6925837320574163"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score against test set\n",
    "modeling_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted 0  Predicted 1\n",
      "Actual 0          307          131\n",
      "Actual 1          126          272\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.70      0.70       438\n",
      "          1       0.67      0.68      0.68       398\n",
      "\n",
      "avg / total       0.69      0.69      0.69       836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict test data\n",
    "pred = modeling_pipe.predict(X_test)\n",
    "\n",
    "# Print confusion matrix and classification report\n",
    "cf_matrix = confusion_matrix(y_test, pred)\n",
    "print(pd.DataFrame(cf_matrix, columns=['Predicted 0', 'Predicted 1'],\\\n",
    "            index=['Actual 0', 'Actual 1']))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that work and we received a slightly worse score than when we predicted with just the diameter as a polynomial feature!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "Pipelines and feature unions can create really robust and powerful way to reproducibly transform and predict models. In cases where we have a need to continually predict a target given the same set of features, pipelines can help really speed up and automate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
